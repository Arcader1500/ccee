[
    {
        "id": "pml_ch4_1",
        "text": "What is 'Support Vector Machine' (SVM)?",
        "options": [
            {
                "id": "a",
                "text": "Regression only model."
            },
            {
                "id": "b",
                "text": "Classifier that finds the hyperplane which maximizes the margin between classes."
            },
            {
                "id": "c",
                "text": "Model for sorting lists."
            },
            {
                "id": "d",
                "text": "Type of computer hardware."
            }
        ],
        "correctOptionId": "b",
        "hint": "Max margin hyperplane.",
        "explanation": "SVM works by finding the widest 'street' between two groups of data."
    },
    {
        "id": "pml_ch4_2",
        "text": "In SVM, what are 'Support Vectors'?",
        "options": [
            {
                "id": "a",
                "text": "The data points closest to the hyperplane."
            },
            {
                "id": "b",
                "text": "The labels."
            },
            {
                "id": "c",
                "text": "The origin (0,0)."
            },
            {
                "id": "d",
                "text": "Incorrect predictions."
            }
        ],
        "correctOptionId": "a",
        "hint": "Points on the edge.",
        "explanation": "These points 'support' the hyperplane; if you move them, the boundary moves."
    },
    {
        "id": "pml_ch4_3",
        "text": "What is the 'Kernel Trick' in SVM?",
        "options": [
            {
                "id": "a",
                "text": "A way to delete data."
            },
            {
                "id": "b",
                "text": "A sorting algorithm."
            },
            {
                "id": "c",
                "text": "Transforming data into higher dimensions to make it linearly separable without actually doing the transform."
            },
            {
                "id": "d",
                "text": "A password."
            }
        ],
        "correctOptionId": "c",
        "hint": "Non-linear separation.",
        "explanation": "Allows SVM to calculate complex boundaries using kernel functions (RBF, Polynomial)."
    },
    {
        "id": "pml_ch4_4",
        "text": "How does K-Nearest Neighbors (KNN) classify a point?",
        "options": [
            {
                "id": "a",
                "text": "Randomly."
            },
            {
                "id": "b",
                "text": "By using a decision tree."
            },
            {
                "id": "c",
                "text": "By calculating a mean."
            },
            {
                "id": "d",
                "text": "By taking a majority vote of its 'K' closest neighbors."
            }
        ],
        "correctOptionId": "d",
        "hint": "Neighborhood vote.",
        "explanation": "It's a lazy learner; it doesn't build a model, just stores the data."
    },
    {
        "id": "pml_ch4_5",
        "text": "KNN is called a 'Lazy Learner' because?",
        "options": [
            {
                "id": "a",
                "text": "It is slow to code."
            },
            {
                "id": "b",
                "text": "It uses less power."
            },
            {
                "id": "c",
                "text": "It is inaccurate."
            },
            {
                "id": "d",
                "text": "It has no training phase; it does all computation during prediction."
            }
        ],
        "correctOptionId": "d",
        "hint": "Instance-based learning.",
        "explanation": "The 'training' is just storing the dataset."
    },
    {
        "id": "pml_ch4_6",
        "text": "What is a 'Decision Tree'?",
        "options": [
            {
                "id": "a",
                "text": "A natural forest."
            },
            {
                "id": "b",
                "text": "A list of rules."
            },
            {
                "id": "c",
                "text": "A database."
            },
            {
                "id": "d",
                "text": "A tree-like model where internal nodes represent tests on features and leaves represent class labels."
            }
        ],
        "correctOptionId": "d",
        "hint": "Flowchart-like structure.",
        "explanation": "Highly interpretable model used for both classification and regression."
    },
    {
        "id": "pml_ch4_7",
        "text": "In a Decision Tree, what is 'Entropy'?",
        "options": [
            {
                "id": "a",
                "text": "Model weight."
            },
            {
                "id": "b",
                "text": "Measure of impurity or randomness in the data at a node."
            },
            {
                "id": "c",
                "text": "Calculating speed."
            },
            {
                "id": "d",
                "text": "Data size."
            }
        ],
        "correctOptionId": "b",
        "hint": "Disorder measurement.",
        "explanation": "If a node has 50% cat and 50% dog, entropy is high."
    },
    {
        "id": "pml_ch4_8",
        "text": "What does 'Information Gain' measure in Decision Trees?",
        "options": [
            {
                "id": "a",
                "text": "Total data size."
            },
            {
                "id": "b",
                "text": "Accuracy."
            },
            {
                "id": "c",
                "text": "Number of leaves."
            },
            {
                "id": "d",
                "text": "Reduction in entropy after a dataset is split on a feature."
            }
        ],
        "correctOptionId": "d",
        "hint": "Value of a split.",
        "explanation": "Trees choose features that provide the highest Information Gain."
    },
    {
        "id": "pml_ch4_9",
        "text": "What is 'Gini Impurity'?",
        "options": [
            {
                "id": "a",
                "text": "A measure of how often a randomly chosen element from the set would be incorrectly labeled."
            },
            {
                "id": "b",
                "text": "A type of gin."
            },
            {
                "id": "c",
                "text": "A sorting error."
            },
            {
                "id": "d",
                "text": "None."
            }
        ],
        "correctOptionId": "a",
        "hint": "Impurity metric.",
        "explanation": "Used by CART algorithm for splitting decision trees; ranges from 0 to 0.5."
    },
    {
        "id": "pml_ch4_10",
        "text": "What is 'Pruning' in Decision Trees?",
        "options": [
            {
                "id": "a",
                "text": "Cutting the tree down."
            },
            {
                "id": "b",
                "text": "Saving labels."
            },
            {
                "id": "c",
                "text": "Removing parts of the tree that provide little power to prevent overfitting."
            },
            {
                "id": "d",
                "text": "Adding more nodes."
            }
        ],
        "correctOptionId": "c",
        "hint": "Simplifying the tree.",
        "explanation": "Reduces tree size and improves its ability to generalize."
    },
    {
        "id": "pml_ch4_11",
        "text": "How does 'Naive Bayes' work?",
        "options": [
            {
                "id": "a",
                "text": "Voting."
            },
            {
                "id": "b",
                "text": "Using distance."
            },
            {
                "id": "c",
                "text": "Uses neural network."
            },
            {
                "id": "d",
                "text": "Uses Bayes Theorem with an assumption of independence between every pair of features."
            }
        ],
        "correctOptionId": "d",
        "hint": "Probabilistic classifier.",
        "explanation": "Excellent for text classification and high-dimensional features."
    },
    {
        "id": "pml_ch4_12",
        "text": "Why is it called 'Naive' Bayes?",
        "options": [
            {
                "id": "a",
                "text": "Written by a naive person."
            },
            {
                "id": "b",
                "text": "Ancient."
            },
            {
                "id": "c",
                "text": "Because it simplifies the problem by assuming features don't interact (independence)."
            },
            {
                "id": "d",
                "text": "Simple code."
            }
        ],
        "correctOptionId": "c",
        "hint": "Independence assumption.",
        "explanation": "In real life, features are often related, but the assumption makes the math easier."
    },
    {
        "id": "pml_ch4_13",
        "text": "What is a 'Logistic Regression'?",
        "options": [
            {
                "id": "a",
                "text": "Regression for numbers."
            },
            {
                "id": "b",
                "text": "Type of tree."
            },
            {
                "id": "c",
                "text": "Classification algorithm that predicts the probability of an input belonging to a class."
            },
            {
                "id": "d",
                "text": "Sorting data."
            }
        ],
        "correctOptionId": "c",
        "hint": "Sigmoid output.",
        "explanation": "Despite the name 'Regression', it is primarily used for classification."
    },
    {
        "id": "pml_ch4_14",
        "text": "What function maps input to probability in Logistic Regression?",
        "options": [
            {
                "id": "a",
                "text": "Log."
            },
            {
                "id": "b",
                "text": "Step Function."
            },
            {
                "id": "c",
                "text": "Sigmoid (Logistic) Function."
            },
            {
                "id": "d",
                "text": "Sine function."
            }
        ],
        "correctOptionId": "c",
        "hint": "Squashes values between 0 and 1.",
        "explanation": "y = 1 / (1 + e^-z)."
    },
    {
        "id": "pml_ch4_15",
        "text": "What is 'Confusion Matrix'?",
        "options": [
            {
                "id": "a",
                "text": "A blurred image."
            },
            {
                "id": "b",
                "text": "A complex spreadsheet."
            },
            {
                "id": "c",
                "text": "A math error."
            },
            {
                "id": "d",
                "text": "Table used to evaluate classification performance showing TN, TP, FN, FP."
            }
        ],
        "correctOptionId": "d",
        "hint": "TP/TN/FP/FN grid.",
        "explanation": "Helpful to see what types of errors (false positives vs false negatives) the model is making."
    },
    {
        "id": "pml_ch4_16",
        "text": "In a Confusion Matrix, what is 'TP'?",
        "options": [
            {
                "id": "a",
                "text": "True Positive (correctly predicted positive)."
            },
            {
                "id": "b",
                "text": "Total Points."
            },
            {
                "id": "c",
                "text": "Top Priority."
            },
            {
                "id": "d",
                "text": "Type Parameter."
            }
        ],
        "correctOptionId": "a",
        "hint": "Correct hit.",
        "explanation": "The model said yes, and it was actually yes."
    },
    {
        "id": "pml_ch4_17",
        "text": "What is a 'False Positive' (Type I error)?",
        "options": [
            {
                "id": "a",
                "text": "Missing a label."
            },
            {
                "id": "b",
                "text": "Correct negative."
            },
            {
                "id": "c",
                "text": "Model predicted positive, but it was actually negative ('False Alarm')."
            },
            {
                "id": "d",
                "text": "Zero error."
            }
        ],
        "correctOptionId": "c",
        "hint": "Wrong alarm.",
        "explanation": "Example: Saying a non-spam email is spam."
    },
    {
        "id": "pml_ch4_18",
        "text": "What is a 'False Negative' (Type II error)?",
        "options": [
            {
                "id": "a",
                "text": "Adding data."
            },
            {
                "id": "b",
                "text": "Model predicted negative, but it was actually positive ('Miss')."
            },
            {
                "id": "c",
                "text": "Correct positive."
            },
            {
                "id": "d",
                "text": "Rounding error."
            }
        ],
        "correctOptionId": "b",
        "hint": "Missing a real case.",
        "explanation": "Example: Failing to detect a disease in a sick patient."
    },
    {
        "id": "pml_ch4_19",
        "text": "What is 'Accuracy' score?",
        "options": [
            {
                "id": "a",
                "text": "TP / (TP + FP)."
            },
            {
                "id": "b",
                "text": "(TP + TN) / Total."
            },
            {
                "id": "c",
                "text": "FN / TP."
            },
            {
                "id": "d",
                "text": "TN / Total."
            }
        ],
        "correctOptionId": "b",
        "hint": "Total correct ratio.",
        "explanation": "Unreliable when data is imbalanced (e.g., 99% of emails are non-spam)."
    },
    {
        "id": "pml_ch4_20",
        "text": "When is 'Precision' more important than 'Recall'?",
        "options": [
            {
                "id": "a",
                "text": "When data is small."
            },
            {
                "id": "b",
                "text": "Never."
            },
            {
                "id": "c",
                "text": "In medical disease detection."
            },
            {
                "id": "d",
                "text": "When the cost of a False Positive is high (e.g., spam filtering)."
            }
        ],
        "correctOptionId": "d",
        "hint": "Avoid False Alarms.",
        "explanation": "You don't want to mark a good email as spam."
    },
    {
        "id": "pml_ch4_21",
        "text": "When is 'Recall' more important than 'Precision'?",
        "options": [
            {
                "id": "a",
                "text": "When we want to be safe."
            },
            {
                "id": "b",
                "text": "When the cost of a False Negative is high (e.g., cancer detection)."
            },
            {
                "id": "c",
                "text": "Only for regression."
            },
            {
                "id": "d",
                "text": "Always."
            }
        ],
        "correctOptionId": "b",
        "hint": "Avoid missing cases.",
        "explanation": "It's better to verify a healthy person again than to let a sick person go untreated."
    },
    {
        "id": "pml_ch4_22",
        "text": "What does a 'Root' node in a Decision Tree represent?",
        "options": [
            {
                "id": "a",
                "text": "A list of labels."
            },
            {
                "id": "b",
                "text": "The very first split founded on the most important feature."
            },
            {
                "id": "c",
                "text": "A math error."
            },
            {
                "id": "d",
                "text": "The end of the tree."
            }
        ],
        "correctOptionId": "b",
        "hint": "Starts the tree.",
        "explanation": "Contains the entire dataset before any splits."
    },
    {
        "id": "pml_ch4_23",
        "text": "What does a 'Leaf' node represent?",
        "options": [
            {
                "id": "a",
                "text": "A final class label (result)."
            },
            {
                "id": "b",
                "text": "The start split."
            },
            {
                "id": "c",
                "text": "Input data."
            },
            {
                "id": "d",
                "text": "A weight."
            }
        ],
        "correctOptionId": "a",
        "hint": "Terminal node.",
        "explanation": "Final decision is made here."
    },
    {
        "id": "pml_ch4_24",
        "text": "Maximum value of Entropy?",
        "options": [
            {
                "id": "a",
                "text": "0"
            },
            {
                "id": "b",
                "text": "1 (for binary classification, 50/50 split)."
            },
            {
                "id": "c",
                "text": "Infinite."
            },
            {
                "id": "d",
                "text": "100"
            }
        ],
        "correctOptionId": "b",
        "hint": "Perfect disorder.",
        "explanation": "If all classes are equally likely, entropy is at its peak."
    },
    {
        "id": "pml_ch4_25",
        "text": "What is the 'Slack Variable' in SVM?",
        "options": [
            {
                "id": "a",
                "text": "Allows SVM to ignore some misclassifications for a cleaner boundary (Soft Margin)."
            },
            {
                "id": "b",
                "text": "A variable in a chat app."
            },
            {
                "id": "c",
                "text": "The weight matrix."
            },
            {
                "id": "d",
                "text": "Zero."
            }
        ],
        "correctOptionId": "a",
        "hint": "Handle outliers.",
        "explanation": "Gives robustness to noisy data that prevents a hard margin split."
    },
    {
        "id": "pml_ch4_26",
        "text": "Normalizing KNN features is important because?",
        "options": [
            {
                "id": "a",
                "text": "It uses more memory."
            },
            {
                "id": "b",
                "text": "It handles text better."
            },
            {
                "id": "c",
                "text": "Euclidean distance is dominated by features with larger scales."
            },
            {
                "id": "d",
                "text": "Model runs faster."
            }
        ],
        "correctOptionId": "c",
        "hint": "Distance calculation bias.",
        "explanation": "Distance between (1,1) and (2,2) is small, but if one feature is in millions, it wins."
    },
    {
        "id": "pml_ch4_27",
        "text": "Typical choice for K in KNN?",
        "options": [
            {
                "id": "a",
                "text": "Zero."
            },
            {
                "id": "b",
                "text": "Odd number (to avoid ties)."
            },
            {
                "id": "c",
                "text": "Equal to number of rows."
            },
            {
                "id": "d",
                "text": "Always 2."
            }
        ],
        "correctOptionId": "b",
        "hint": "Majority vote resolution.",
        "explanation": "Usually found using cross-validation; small K is sensitive to noise."
    },
    {
        "id": "pml_ch4_28",
        "text": "Decision trees perform automatic?",
        "options": [
            {
                "id": "a",
                "text": "Scaling."
            },
            {
                "id": "b",
                "text": "Hyperparameter tuning."
            },
            {
                "id": "c",
                "text": "Feature Selection."
            },
            {
                "id": "d",
                "text": "Data collection."
            }
        ],
        "correctOptionId": "c",
        "hint": "Node selection.",
        "explanation": "They only use the features that actually help split the data."
    },
    {
        "id": "pml_ch4_29",
        "text": "In Logistic Regression, output of 0.7 means?",
        "options": [
            {
                "id": "a",
                "text": "70% probability the sample belongs to Class 1."
            },
            {
                "id": "b",
                "text": "Confidence is 7."
            },
            {
                "id": "c",
                "text": "Error is 30%."
            },
            {
                "id": "d",
                "text": "The value is 0.7."
            }
        ],
        "correctOptionId": "a",
        "hint": "Probability interpretation.",
        "explanation": "Threshold (usually 0.5) is applied to get a binary label."
    },
    {
        "id": "pml_ch4_20",
        "text": "SVM is primarily for?",
        "options": [
            {
                "id": "a",
                "text": "Clustering."
            },
            {
                "id": "b",
                "text": "Finding averages."
            },
            {
                "id": "c",
                "text": "Sorting."
            },
            {
                "id": "d",
                "text": "Binary Classification (though can be extended)."
            }
        ],
        "correctOptionId": "d",
        "hint": "Two-group separator.",
        "explanation": "Can handle multiple classes using One-vs-One or One-vs-Rest strategies."
    }
]