[
    {
        "id": "pml_ch2_1",
        "text": "What is 'Clustering'?",
        "options": [
            {
                "id": "a",
                "text": "Predicting continuous numerical values"
            },
            {
                "id": "b",
                "text": "Assigning predefined labels to data points"
            },
            {
                "id": "c",
                "text": "Grouping objects so those in the same group are more similar than those in other groups"
            },
            {
                "id": "d",
                "text": "Sorting data in chronological order"
            }
        ],
        "correctOptionId": "c",
        "hint": "Unsupervised task.",
        "explanation": "No labels are provided; the algorithm finds natural patterns."
    },
    {
        "id": "pml_ch2_2",
        "text": "How does K-Means Clustering work?",
        "options": [
            {
                "id": "a",
                "text": "It builds a hierarchical tree structure"
            },
            {
                "id": "b",
                "text": "It uses a voting system from nearest neighbors"
            },
            {
                "id": "c",
                "text": "It iteratively moves centroids to cluster centers and reassigns points"
            },
            {
                "id": "d",
                "text": "It calculates gradients for optimization"
            }
        ],
        "correctOptionId": "c",
        "hint": "Centroid-based.",
        "explanation": "Highly efficient but requires pre-specifying 'K'."
    },
    {
        "id": "pml_ch2_3",
        "text": "What is 'K' in K-Means?",
        "options": [
            {
                "id": "a",
                "text": "Number of features in the dataset"
            },
            {
                "id": "b",
                "text": "A constant value always set to 10"
            },
            {
                "id": "c",
                "text": "Total number of data points"
            },
            {
                "id": "d",
                "text": "Number of clusters desired"
            }
        ],
        "correctOptionId": "d",
        "hint": "Hyperparameter.",
        "explanation": "Determines how many groups the data will be split into."
    },
    {
        "id": "pml_ch2_4",
        "text": "What is 'Hierarchical Clustering'?",
        "options": [
            {
                "id": "a",
                "text": "A clustering method designed for management hierarchies"
            },
            {
                "id": "b",
                "text": "An optimized version of K-Means for speed"
            },
            {
                "id": "c",
                "text": "A method that builds a hierarchy of clusters shown as a Dendrogram"
            },
            {
                "id": "d",
                "text": "A flat clustering with no structure"
            }
        ],
        "correctOptionId": "c",
        "hint": "Tree-like structure.",
        "explanation": "Can be Agglomerative (bottom-up) or Divisive (top-down)."
    },
    {
        "id": "pml_ch2_5",
        "text": "What is a 'Dendrogram'?",
        "options": [
            {
                "id": "a",
                "text": "A tree diagram representing hierarchical clustering arrangement"
            },
            {
                "id": "b",
                "text": "A type of scatter plot for regression"
            },
            {
                "id": "c",
                "text": "A circular pie chart visualization"
            },
            {
                "id": "d",
                "text": "A unit of computer memory"
            }
        ],
        "correctOptionId": "a",
        "hint": "Visualizing hierarchy.",
        "explanation": "Shows at what distance clusters were merged."
    },
    {
        "id": "pml_ch2_6",
        "text": "What is 'DBSCAN'?",
        "options": [
            {
                "id": "a",
                "text": "Deep Binary Scanning Algorithm"
            },
            {
                "id": "b",
                "text": "Database Scanning for Clusters"
            },
            {
                "id": "c",
                "text": "Density-Based Spatial Clustering of Applications with Noise"
            },
            {
                "id": "d",
                "text": "Distance-Based Scanning Method"
            }
        ],
        "correctOptionId": "c",
        "hint": "Density-based.",
        "explanation": "Finds arbitrarily shaped clusters and identifies noise (outliers)."
    },
    {
        "id": "pml_ch2_7",
        "text": "Advantage of DBSCAN over K-Means?",
        "options": [
            {
                "id": "a",
                "text": "Doesn't require pre-specifying number of clusters"
            },
            {
                "id": "b",
                "text": "Can find non-spherical clusters"
            },
            {
                "id": "c",
                "text": "Identifies outliers automatically"
            },
            {
                "id": "d",
                "text": "All of the above"
            }
        ],
        "correctOptionId": "d",
        "hint": "Robust clustering.",
        "explanation": "DBSCAN is much more flexible for complex distributions."
    },
    {
        "id": "pml_ch2_8",
        "text": "In DBSCAN, what is 'Epsilon' (eps)?",
        "options": [
            {
                "id": "a",
                "text": "The total number of clusters to create"
            },
            {
                "id": "b",
                "text": "Maximum distance between samples for neighborhood consideration"
            },
            {
                "id": "c",
                "text": "The initial starting point for clustering"
            },
            {
                "id": "d",
                "text": "The acceptable error rate threshold"
            }
        ],
        "correctOptionId": "b",
        "hint": "Radius of neighborhood.",
        "explanation": "Controls the 'reach' of the density search."
    },
    {
        "id": "pml_ch2_9",
        "text": "What is 'WCSS' (Within-Cluster Sum of Squares)?",
        "options": [
            {
                "id": "a",
                "text": "Sum of squared distances between each point and its centroid"
            },
            {
                "id": "b",
                "text": "Total variance across all features"
            },
            {
                "id": "c",
                "text": "Mean value of all clusters"
            },
            {
                "id": "d",
                "text": "Classification accuracy score"
            }
        ],
        "correctOptionId": "a",
        "hint": "Measure of compactness.",
        "explanation": "Used in the Elbow Method to evaluate K-Means."
    },
    {
        "id": "pml_ch2_10",
        "text": "What is the 'Silhouette Score'?",
        "options": [
            {
                "id": "a",
                "text": "Count of empty clusters in the result"
            },
            {
                "id": "b",
                "text": "Measure of how similar an object is to its own cluster vs other clusters"
            },
            {
                "id": "c",
                "text": "Execution speed of the clustering algorithm"
            },
            {
                "id": "d",
                "text": "The maximum depth of a decision tree"
            }
        ],
        "correctOptionId": "b",
        "hint": "Distance between clusters.",
        "explanation": "Ranges from -1 (wrong) to +1 (perfectly matched)."
    },
    {
        "id": "pml_ch2_11",
        "text": "What is 'Agglomerative' clustering?",
        "options": [
            {
                "id": "a",
                "text": "Top-down approach starting with one cluster"
            },
            {
                "id": "b",
                "text": "Random assignment of cluster labels"
            },
            {
                "id": "c",
                "text": "Bottom-up approach where each point starts as a cluster and merges"
            },
            {
                "id": "d",
                "text": "Using only centroids for clustering"
            }
        ],
        "correctOptionId": "c",
        "hint": "Merging clusters.",
        "explanation": "Most common type of hierarchical clustering."
    },
    {
        "id": "pml_ch2_12",
        "text": "What is 'Linkage' in hierarchical clustering?",
        "options": [
            {
                "id": "a",
                "text": "Connecting computers in a network"
            },
            {
                "id": "b",
                "text": "The rule to calculate distance between clusters (e.g., ward, single, complete)"
            },
            {
                "id": "c",
                "text": "Loading data from external sources"
            },
            {
                "id": "d",
                "text": "Removing clusters from the result"
            }
        ],
        "correctOptionId": "b",
        "hint": "How to measure cluster distance.",
        "explanation": "'Single' uses the closest points; 'Complete' uses the furthest."
    },
    {
        "id": "pml_ch2_13",
        "text": "K-Means is sensitive to?",
        "options": [
            {
                "id": "a",
                "text": "Outliers in the data"
            },
            {
                "id": "b",
                "text": "Initial choice of centroids"
            },
            {
                "id": "c",
                "text": "Scaling of features"
            },
            {
                "id": "d",
                "text": "All of the above"
            }
        ],
        "correctOptionId": "d",
        "hint": "Weaknesses of K-Means.",
        "explanation": "Needs careful initialization (like K-Means++) and normalized data."
    },
    {
        "id": "pml_ch2_14",
        "text": "What is a 'Centroid'?",
        "options": [
            {
                "id": "a",
                "text": "A type of autonomous robot"
            },
            {
                "id": "b",
                "text": "The first data point in the dataset"
            },
            {
                "id": "c",
                "text": "The center of a cluster (average of all points in it)"
            },
            {
                "id": "d",
                "text": "A rejected or outlier point"
            }
        ],
        "correctOptionId": "c",
        "hint": "Geographic center.",
        "explanation": "Represents the cluster prototype."
    },
    {
        "id": "pml_ch2_15",
        "text": "If WCSS drops sharply and then slowly, the sharp bend is called the?",
        "options": [
            {
                "id": "a",
                "text": "Knee point"
            },
            {
                "id": "b",
                "text": "Elbow point"
            },
            {
                "id": "c",
                "text": "Spine location"
            },
            {
                "id": "d",
                "text": "Edge threshold"
            }
        ],
        "correctOptionId": "b",
        "hint": "Optimization graph.",
        "explanation": "Indicates the point of diminishing return for K."
    },
    {
        "id": "pml_ch2_16",
        "text": "What is 'Mean Shift' clustering?",
        "options": [
            {
                "id": "a",
                "text": "Shifting all data values to zero"
            },
            {
                "id": "b",
                "text": "A type of linear regression"
            },
            {
                "id": "c",
                "text": "A sliding-window algorithm that finds dense areas of data points"
            },
            {
                "id": "d",
                "text": "None of the clustering methods"
            }
        ],
        "correctOptionId": "c",
        "hint": "Centroid shifting.",
        "explanation": "It iterates to find the mode of a density distribution."
    },
    {
        "id": "pml_ch2_17",
        "text": "In DBSCAN, a 'Core Point' is?",
        "options": [
            {
                "id": "a",
                "text": "The geographic center of the world"
            },
            {
                "id": "b",
                "text": "The very first point in the dataset"
            },
            {
                "id": "c",
                "text": "A point with at least 'min_samples' in its neighborhood"
            },
            {
                "id": "d",
                "text": "An outlier or noise point"
            }
        ],
        "correctOptionId": "c",
        "hint": "Dense point.",
        "explanation": "Starts a new cluster or expands one."
    },
    {
        "id": "pml_ch2_18",
        "text": "In DBSCAN, a 'Noise Point' is?",
        "options": [
            {
                "id": "a",
                "text": "A point that generates excessive sound"
            },
            {
                "id": "b",
                "text": "A point with value exactly zero"
            },
            {
                "id": "c",
                "text": "The center point of a cluster"
            },
            {
                "id": "d",
                "text": "A point neither core nor reachable from any core point"
            }
        ],
        "correctOptionId": "d",
        "hint": "Outlier.",
        "explanation": "These are the 'noise' that the algorithm ignores."
    },
    {
        "id": "pml_ch2_19",
        "text": "What is 'Medoid' (in K-Medoids)?",
        "options": [
            {
                "id": "a",
                "text": "The calculated average point of a cluster"
            },
            {
                "id": "b",
                "text": "A randomly selected point"
            },
            {
                "id": "c",
                "text": "The ACTUAL data point closest to all other points in a cluster"
            },
            {
                "id": "d",
                "text": "A type of medical treatment"
            }
        ],
        "correctOptionId": "c",
        "hint": "Real point vs Mean.",
        "explanation": "More robust to outliers than K-Means."
    },
    {
        "id": "pml_ch2_20",
        "text": "Most common distance metric for K-Means?",
        "options": [
            {
                "id": "a",
                "text": "Euclidean Distance"
            },
            {
                "id": "b",
                "text": "Manhattan Distance"
            },
            {
                "id": "c",
                "text": "Hamming Distance"
            },
            {
                "id": "d",
                "text": "Cosine Similarity"
            }
        ],
        "correctOptionId": "a",
        "hint": "Straight line distance.",
        "explanation": "Standard measure in many-dimensional numeric space."
    },
    {
        "id": "pml_ch2_21",
        "text": "Can K-Means work with categorical data directly?",
        "options": [
            {
                "id": "a",
                "text": "Yes, it works with all data types"
            },
            {
                "id": "b",
                "text": "Only with binary categorical data"
            },
            {
                "id": "c",
                "text": "Maybe, depending on the implementation"
            },
            {
                "id": "d",
                "text": "No, because 'mean' of categories is not defined"
            }
        ],
        "correctOptionId": "d",
        "hint": "Mathematical limitation.",
        "explanation": "Requires encoding or specialized variants like K-Modes."
    },
    {
        "id": "pml_ch2_22",
        "text": "What is 'Spectral Clustering'?",
        "options": [
            {
                "id": "a",
                "text": "Clustering designed for paranormal detection"
            },
            {
                "id": "b",
                "text": "Clustering based on light wave frequencies"
            },
            {
                "id": "c",
                "text": "Using eigenvalues of similarity matrix for dimensionality reduction before clustering"
            },
            {
                "id": "d",
                "text": "Web search and indexing algorithm"
            }
        ],
        "correctOptionId": "c",
        "hint": "Graph-based clustering.",
        "explanation": "Useful for identifying clusters based on connectivity rather than just distance."
    },
    {
        "id": "pml_ch2_23",
        "text": "What is 'Curse of Dimensionality' in clustering?",
        "options": [
            {
                "id": "a",
                "text": "Excessive memory usage during processing"
            },
            {
                "id": "b",
                "text": "Distance between points becomes almost uniform, making clusters hard to find"
            },
            {
                "id": "c",
                "text": "Model crashes due to complexity"
            },
            {
                "id": "d",
                "text": "Sorting errors in high dimensions"
            }
        ],
        "correctOptionId": "b",
        "hint": "Sparse data issue.",
        "explanation": "As dimensionality increases, the concept of 'closest' neighbor fades."
    },
    {
        "id": "pml_ch2_24",
        "text": "'GMM' (Gaussian Mixture Model) is different from K-Means because?",
        "options": [
            {
                "id": "a",
                "text": "It executes significantly faster"
            },
            {
                "id": "b",
                "text": "It uses discrete assignments only"
            },
            {
                "id": "c",
                "text": "It uses probabilities (Soft assignment) instead of hard labels"
            },
            {
                "id": "d",
                "text": "It uses tree-based structures"
            }
        ],
        "correctOptionId": "c",
        "hint": "Probability based.",
        "explanation": "GMM assumes points come from multiple normal distributions overlapping."
    },
    {
        "id": "pml_ch2_25",
        "text": "In hierarchical clustering, what is 'Divisive'?",
        "options": [
            {
                "id": "a",
                "text": "Bottom-up merging approach"
            },
            {
                "id": "b",
                "text": "Mathematical division operation"
            },
            {
                "id": "c",
                "text": "Top-down approach where all points start in one cluster and split"
            },
            {
                "id": "d",
                "text": "Deleting data systematically"
            }
        ],
        "correctOptionId": "c",
        "hint": "Splitting hierarchy.",
        "explanation": "Starts with everyone in a single large group."
    },
    {
        "id": "pml_ch2_26",
        "text": "What is 'Balanced iterative reducing and clustering using hierarchies' (BIRCH)?",
        "options": [
            {
                "id": "a",
                "text": "A type of decision tree"
            },
            {
                "id": "b",
                "text": "A memory-efficient clustering algorithm for very large datasets"
            },
            {
                "id": "c",
                "text": "A sorting methodology"
            },
            {
                "id": "d",
                "text": "A database management tool"
            }
        ],
        "correctOptionId": "b",
        "hint": "Large scale clustering.",
        "explanation": "Designed to handle massive data by building a tree summarize."
    },
    {
        "id": "pml_ch2_27",
        "text": "Complexity of K-Means is roughly?",
        "options": [
            {
                "id": "a",
                "text": "Linear with number of points O(n)"
            },
            {
                "id": "b",
                "text": "Exponential O(e^n)"
            },
            {
                "id": "c",
                "text": "Logarithmic O(log n)"
            },
            {
                "id": "d",
                "text": "Infinite complexity"
            }
        ],
        "correctOptionId": "a",
        "hint": "Efficient algorithm.",
        "explanation": "It's very scalable for millions of points."
    },
    {
        "id": "pml_ch2_28",
        "text": "What is a 'Flat' clustering?",
        "options": [
            {
                "id": "a",
                "text": "Clusters with zero depth"
            },
            {
                "id": "b",
                "text": "Empty clustering result"
            },
            {
                "id": "c",
                "text": "Clusters without hierarchical relationship (like K-Means)"
            },
            {
                "id": "d",
                "text": "Flat file storage format"
            }
        ],
        "correctOptionId": "c",
        "hint": "Direct partitioning.",
        "explanation": "The output is just a list of cluster labels."
    },
    {
        "id": "pml_ch2_29",
        "text": "What refers to 'Compactness' in clustering?",
        "options": [
            {
                "id": "a",
                "text": "Total number of data points"
            },
            {
                "id": "b",
                "text": "How close the points in a cluster are to the centroid"
            },
            {
                "id": "c",
                "text": "The file size of the dataset"
            },
            {
                "id": "d",
                "text": "Processing speed of the algorithm"
            }
        ],
        "correctOptionId": "b",
        "hint": "Density.",
        "explanation": "High compactness usually means a 'better' cluster."
    },
    {
        "id": "pml_ch2_30",
        "text": "What refers to 'Separation' in clustering?",
        "options": [
            {
                "id": "a",
                "text": "Splitting files into parts"
            },
            {
                "id": "b",
                "text": "Removing data from the dataset"
            },
            {
                "id": "c",
                "text": "How well distinct clusters are separated from each other"
            },
            {
                "id": "d",
                "text": "Deleting source code"
            }
        ],
        "correctOptionId": "c",
        "hint": "Inter-cluster distance.",
        "explanation": "High separation ensures clusters are distinct and not overlapping too much."
    }
]